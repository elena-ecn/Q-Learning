{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q-Learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNq36L3LWNMZ0h7hQTZEywW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"q0kUo5HLH7BY","executionInfo":{"status":"ok","timestamp":1635791232269,"user_tz":-120,"elapsed":3663,"user":{"displayName":"Elena Oikonomou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdjieaR8SwyW3oT5Gj_-eTKl6SVjziMwjwjye6=s64","userId":"03571600125738632827"}},"outputId":"69255a6b-cf83-49d7-eca1-c63708c4ff92"},"source":["\"\"\"\n","     *** Reinforcement Learning in Robotics ***\n","     ------------------------------------------\n","    Train an agent on a gridworld with Q-Learning.\n","\n","Date:   Fall 2021\n","Author: elena_ecn \n","\"\"\"\n","\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","\n","\n","class Env:\n","    def __init__(self, rows, columns):      \n","        self.rows = rows                                    # Number of rows of gridworld\n","        self.columns = columns                              # Number of columns of gridworld\n","        self.num_states = rows*columns                      # Number of states\n","        self.actions = self.define_actions()                # Define the actions the agent can take\n","        self.num_actions = len(self.actions)                # Number of actions the agent can take\n","        self.rewards = self.define_rewards()                # Define the reward for each state\n","        self.obstacles = self.define_obstacles()            # Define the obstacles of the environment\n","        self.states = [(i,j) for i in range(rows) for j in range(columns)]  # State coordinates   \n","        self.directions = ['front', 'to_left', 'to_right']  # Non deterministic dynamics\n","        self.distribution = [.8, .1, .1]                    # Probability distribution of dynamics            \n","    \n","    def define_actions(self):\n","        \"\"\"Define the possible actions that the agent can take.\"\"\"\n","        actions = ['up', 'right', 'down', 'left']  # 0 = up, 1 = right, 2 = down, 3 = left       \n","        return actions   \n","    \n","    def define_rewards(self):\n","        \"\"\"Define the reward for each state.\"\"\"\n","        rewards = -0.04*np.ones([rows, columns])\n","        rewards[0, 3] = 1.0\n","        rewards[1, 3] = -1.0      \n","        return rewards\n","    \n","    def define_obstacles(self):\n","        \"\"\"Define the obstacles of the environment and set their rewards to None.\"\"\"\n","        obstacles = [(1,1)]           # Locations of obstacles\n","        for obs in obstacles:\n","            self.rewards[obs] = None  # Set reward of obstacle locations to None\n","        return obstacles\n","\n","    def is_terminal_state(self, state):\n","        \"\"\"Determines whether the state is terminal. \n","        \n","        Inputs:\n","          - state(tuple): A state of the environment\n","        Returns:\n","          - (bool):       Whether the state is terminal or not\n","        \"\"\"    \n","\n","        if self.rewards[state] == -1.0 or self.rewards[state] == 1.0:\n","            return True \n","        else:\n","            return False\n","    \n","    def get_next_state(self, state, action):\n","        \"\"\"Get the next state based on the current state and the chosen action.\n","        \n","        The dynamics are non-deterministic and the agent transitions to the desired \n","        state with a high probability but might slip to the left or to the right.\n","        Inputs:\n","          - state(tuple): The current state of the agent\n","          - action(int):  The chosen action to take\n","        Returns:\n","          - (tuple):      The next state of the agent\n","        \"\"\"\n","        action_word = self.actions[action]  # Get the action in text form\n","\n","        # Adjust action if agent 'slips'\n","        non_determ_direction = random.choices(self.directions, self.distribution)\n","        if non_determ_direction == 'to_left':\n","            if action_word == 'up':\n","                action_word = 'left'\n","            elif action_word == 'right':\n","                action_word = 'up'\n","            elif action_word == 'down':\n","                action_word = 'right'\n","            elif action_word == 'left':\n","                action_word = 'down'\n","        elif non_determ_direction == 'to_right':\n","            if action_word == 'up':\n","                action_word = 'right'\n","            elif action_word == 'right':\n","                action_word = 'down'\n","            elif action_word == 'down':\n","                action_word = 'left'\n","            elif action_word == 'left':\n","                action_word = 'up'\n","\n","        # Make permissible movement\n","        row, column = state\n","        if action_word == 'up' and row > 0 and ((row-1, column) not in self.obstacles):        \n","            row -= 1     # If agent is not at the upper wall or at obstacle, move up.\n","        elif action_word == 'right' and column < self.columns - 1 and ((row, column+1) not in self.obstacles):\n","            column += 1  # If agent is not at the right wall or at obstacle, move right.\n","        elif action_word == 'down' and row < self.rows - 1 and ((row+1, column) not in self.obstacles):\n","            row += 1     # If agent is not at the bottom wall or at obstacle, move down.\n","        elif action_word == 'left' and column > 0 and ((row, column-1) not in self.obstacles):\n","            column -= 1  # If agent is not at the left wall or at obstacle, move left.\n","\n","        return (row, column)  # Next state \n","\n","\n","class Agent:\n","    def __init__(self, env, start_state, a, epsilon): \n","        self.env = env                        # The environment class\n","        self.num_actions = env.num_actions    # Number of actions the agent can take\n","        self.start_state = start_state        # The state that the agent begins in\n","        self.epsilon = epsilon                # Exploration probability\n","        self.q_table = self.define_q_table()  # The Q-table\n","        self.a = a                            # Learning rate\n","\n","    def define_q_table(self):\n","        \"\"\"Initializes the Q-table arbitrarily except Q(terminal,.)=0.\"\"\"\n","        q_table = np.random.uniform(low=0.01, high=1.0, size=(self.env.num_states, self.num_actions))\n","        \n","        for state in self.env.states:\n","            if self.env.is_terminal_state(state):\n","                state_idx = self.env.states.index(state)                \n","                q_table[state_idx, :] = 0\n","        return q_table\n","\n","    def choose_action(self, state):\n","        \"\"\"Choose next action to take from current state with epsilon-greedy policy.\n","\n","        Explore the environment with probability epsilon and\n","        Exploit knowledge and take greedy action with probability (1-epsilon).\n","        Inputs:\n","          - state(tuple): The current state of the agent\n","        Returns:\n","          - action(int):  The next action to take\n","        \"\"\"\n","\n","        if np.random.random() < self.epsilon:         \n","            action = np.random.randint(self.num_actions)  # Explore environment      \n","        else: \n","            state_idx = self.env.states.index(state)      # State index\n","            action = np.argmax(self.q_table[state_idx])   # Exploit learned values\n","        return action    \n","    \n","    def choose_action_trained(self, state):\n","        \"\"\"Choose the best next action to take from the current state.\n","\n","        Inputs:\n","          - state(tuple): The current state of the agent\n","        Returns:\n","          - (int):        The next action to take\n","        \"\"\"\n","        state_idx = self.env.states.index(state)          # State index\n","        return np.argmax(self.q_table[state_idx])         # Best action\n","\n","    def get_path(self, init_state):\n","        \"\"\"Get the path from the initial state to a terminal state.\n","        \n","        Inputs:\n","          - init_state(tuple): The initial state that the agent begins in\n","        Returns:\n","          - path(list):        The sequence of states the agent takes\n","        \"\"\"\n","\n","        state = init_state\n","        path = [state]\n","        while not self.env.is_terminal_state(state):\n","            action = self.choose_action_trained(state)            \n","            state = self.env.get_next_state(state, action)\n","            path.append(state)\n","        return path\n","    \n","    def train(self, init_state, episodes):\n","        \"\"\"Trains the agent via Q-Learning.\n","        \n","        Updates the Q-table with the optimal policy.\n","        Inputs:\n","          - init_state(tuple): The initial state that the agent begins in\n","          - episodes(int):     Number of episodes to train for\n","        \"\"\"\n","\n","        gamma = 1.0      # Discount factor for future rewards\n","\n","        for episode in range(episodes):\n","            state = init_state  # Initialize/reset state\n","\n","            while not self.env.is_terminal_state(state):\n","                \n","                # Choose action to take\n","                action = self.choose_action(state)  \n","\n","                # Take chosen action and transition to the next state \n","                next_state = self.env.get_next_state(state, action)\n","                \n","                # Receive the reward \n","                reward = self.env.rewards[next_state]                \n","\n","                # Update Q-value \n","                state_idx = self.env.states.index(state)            # State index\n","                next_state_idx = self.env.states.index(next_state)  # Next state index\n","                q_value = self.q_table[state_idx, action]\n","                self.q_table[state_idx, action] = q_value + self.a*(reward + gamma*np.max(self.q_table[next_state_idx]) - q_value)\n","                \n","                # Update state\n","                state = next_state\n","            \n","        print('Training complete!\\n')\n","\n","\n","def plot_path(path, q_table, env):\n","    \"\"\"Plots the optimal path in the gridworld environment.\n","    \n","    Inputs:\n","      - path(list):          The optimal path\n","      - q_table(np.ndarray): The Q-table\n","      - env(class.Env):      The environment\n","    \"\"\"\n","    rows = env.rows\n","    columns = env.columns\n","            \n","    fig, ax = plt.subplots(figsize = (12,8))\n","    ax.set_title('Gridworld')\n","    plt.axis('off')\n","\n","    # Grid\n","    for row in range(rows+1):\n","        ax.hlines(y=row, xmin=0, xmax=columns, linewidth=1.5, color='k')    \n","    for column in range(columns+1):\n","        ax.vlines(x=column, ymin=0, ymax=rows, linewidth=1.5, color='k')\n","    \n","    # Obstacles\n","    for obs in env.obstacles:\n","        row, col = obs\n","        ax.add_patch(Rectangle((col, rows-1-row), 1, 1, color='k'))\n","\n","    # Annotate terminal states (hard-coded)\n","    ax.annotate('+1', xy=(3.35, 2.4),color='#d42436', size='30')\n","    ax.annotate('-1', xy=(3.4, 1.4),color='#d42436', size='30')\n","\n","    # Show path\n","    for state in path:\n","        row, col = state\n","        ax.add_patch(Rectangle((col, rows-1-row), 1, 1, color=\"#104fb5\", alpha=0.2))\n","    \n","    # Optimal Policy (best action arrows)\n","    optimal_actions = [np.argmax(row) for row in q_table]\n","    for i in range(rows):\n","        for j in range(columns):   \n","\n","            state_idx = env.states.index((i, j))   # State index\n","            if optimal_actions[state_idx] == 0:    # up\n","                dx, dy = 0, 0.2\n","            elif optimal_actions[state_idx] == 1:  # right\n","                dx, dy = 0.2, 0\n","            elif optimal_actions[state_idx] == 2:  # down\n","                dx, dy = 0, -0.2\n","            else:                                  # left\n","                dx, dy = -0.2, 0\n","            \n","            if env.is_terminal_state((i, j)):\n","                break  # If terminal state, don't draw arrow\n","            plt.arrow(j+0.5, rows-1-i+0.5, dx, dy, head_width=0.05, head_length=0.1, fc='k', ec='k')\n","   \n","    plt.show()\n","\n","\n","def show_q_table(q_table):\n","    \"\"\"Prints the Q-table in a more readable format.\"\"\"\n","\n","    print('\\t'*5 + 'Q-table \\n' + '-'*50)\n","    print('\\t Up \\t\\t Right \\t\\t Down \\t Left\\n' + '-'*50 )    \n","    rows, columns = q_table.shape\n","    for i in range(rows):\n","        for j in range(columns):\n","            print('{:10.4f}'.format(q_table[i, j]), end=' ')\n","        print('\\n')\n","    print('-'*50 + '\\n')\n","\n","\n","if __name__ == \"__main__\":\n","\n","    np.random.seed(0)    # To make runs repeatable\n","\n","    # Parameters\n","    # -------------------------------------------------------------------------\n","    rows = 3             # Number of rows of the gridworld \n","    columns = 4          # Number of columns of the gridworld\n","    start_state = (2,0)  # State the agent begins in\n","    eps = 0.1            # Exploration probability\n","    N = 20000            # Number of episodes to train for\n","    a = 0.1              # Learning rate\n","\n","    # Train the agent\n","    # -------------------------------------------------------------------------\n","    env = Env(rows, columns)                                   # Define environment\n","    agent = Agent(env, start_state, a, eps)                    # Define agent\n","    agent.train(start_state, N)                                # Train the AI agent\n","            \n","    # Display results\n","    # -------------------------------------------------------------------------\n","    show_q_table(agent.q_table)                                # Display final Q-table\n","    optimal_path = agent.get_path(start_state)                 # Find optimal path\n","    print('The optimal path is: \\n{}\\n'.format(optimal_path))  # Display optimal path       \n","\n","    # Plot the optimal path in the gridworld environment\n","    plot_path(optimal_path, agent.q_table, env)   \n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training complete!\n","\n","\t\t\t\t\tQ-table \n","--------------------------------------------------\n","\t Up \t\t Right \t\t Down \t Left\n","--------------------------------------------------\n","    0.8800     0.9200     0.8400     0.8800 \n","\n","    0.9200     0.9600     0.9200     0.8800 \n","\n","    0.9600     1.0000     0.9200     0.9200 \n","\n","    0.0000     0.0000     0.0000     0.0000 \n","\n","    0.8800     0.8400     0.8000     0.8400 \n","\n","    0.9788     0.8012     0.4669     0.7827 \n","\n","    0.9600    -0.8543     0.7200     0.5916 \n","\n","    0.0000     0.0000     0.0000     0.0000 \n","\n","    0.8400     0.7600     0.8000     0.8000 \n","\n","    0.6961     0.7011     0.7461     0.8000 \n","\n","    0.5947     0.4483     0.6294     0.7600 \n","\n","    0.0958     0.2187     0.1945     0.6318 \n","\n","--------------------------------------------------\n","\n","The optimal path is: \n","[(2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3)]\n","\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]}]}